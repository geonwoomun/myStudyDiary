## TF-IDF (Term Frequency - Inverse Document Frequency)

TF-IDF는 정보 검색과 텍스트 마이닝에서 이용하는 가중치로, 여러 문서로 이루어진 문서군이 있을 때 어떤 단어가 특정 문서 내에서 얼마나 중요한 것인지를 나타내는 통계적 수치이다. 문서의 핵심어를 추출하거나, 검색 엔진에서 검색 결과의 순위를 결정하거나, 문서들 사이의 비슷한 정도를 구하는 등의 용도로 사용할 수 있다.

TF(단어 빈도, term frquency)는 특정한 단어가 문서 내에 얼마나 자주 등장하는지를 나타내는 값으로, 이 값이 높을수록 문서에서 중요하다고 생각할 수 있다. 하지만 단어 자체가 문서군 내에서 자주 사용되는 경우, 이것은 그 단어가 흔하게 등장한다는 것을 의미한다. 이것을 DF(문서 빈도, document frequency)라고 하며, 이값의 역수를 IDF(역문서 빈도, inverse document frequency)라고 한다. TF-IDF는 TF 와 IDF를 곱한 값이다.

IDF 값은 문서군의 성격에 따라 결정된다. 예를 들어 '원자'라는 낱말은 일반적인 문서들 사이에서는 잘 나오지 않기 때문에 IDF 값이 높아지고 문서의 핵심어가 될 수 있지만, 원자에 대한 문서를 모아놓은 문서군의 경우 이 낱말은 상투어가 되어 각 문서들을 세분화하여 구분할 수 있는 다른 낱말들이 높은 가중치를 얻게 된다.

-------------------------------------------------------------------

TF-IDF는 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들 마다 중요한 정도를 가중치로 주는 방법이다. 사용 방법은 우선 DTM을 만든 후, TF-IDF 가중치를 부여합니다.

TF-IDF는 주로 문서의 유서도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 쓰일 수 있다.

TF-IDF는 TF와 IDF를 곱한 값을 의미하는데 이를 식으로 표현해보겠습니다. 문서를 d, 단어를 t, 문서의 총 개수를 n이라고 표현할 때 TF,DF,IDF는 다음과 같이 정의할 수 있다.

### tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수.
 생소한 글자 때문에 어려워 보일 수 있지만, 잘 생각해보면 TF는 이미 앞에서 구한적이 있습니다. TF는 앞에서 배운 DTM의 예제에서 각 단어들이 가진 값들입니다. DTM이 각 문서에서의 각 단어의 등장 빈도를 나타내는 값이었기 때문입니다.

### df(t): 특정 단어 t가 등장한 문서의 수

 여기서 특정 단어가 각 문서, 또는 문서들에서 몇 번 등장했는지는 관심가지지 않으며 오직 특정 단어 t가 등장한 문서의 수에만 관심을 가진다. 바나나라는 단어가 문서 2와 문서3에서 등장했다. 이 경우, 바나나의 df는 2이다. 문서 3에서 바나나가 두 번 등자하지만, 그것은 중요한게 아님. 심지어 바나나란 단어가 문서2에서 100번 등장하고, 문서 3에서 300번 등장한다고 하더라도 그것은 중요한 것이 아님.

### idf(d, t): df(t)에 반비례 하는 수

idf(d, t) = log(n/(1+df(t)))

IDF라는 이름을 보고 DF의 역수가 아닐까 생각했다면, IDF는 DF의 역수를 취하고 싶은 것이 맞다. 그런데 log와 분모에 1을 더해주는 식에 의아할 수 있다. log를 사용하지 않았을 때, IDF를 DF의 역수 (n/df(t))라는 식으로 사용한다면 총 문서 n이 커질 수록, IDF의 값은 기하급수적으로 커지게 된다. 그렇기 때문에 log를 사용한다.

TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단합니다. TF-IDF 값이 낮으면 중요도가 낮은 것이며, TF-IDF 값이 크면 중요도가 큰 것입니다. 즉, the나 a와 같이 불용어의 경우에는 모든 문서에 자주 등장하기 마련이기 때문에 자연스럽게 불용어의 TF-IDF 값은 다른 단어의 TF-IDF에 비해서 낮아지게 됩니다.

-	    과일이	길고	노란	먹고	바나나	사과	싶은	저는	좋아요
문서1	 0	     0	     0	     1	     0	    1	    1	   0	   0
문서2	 0	     0	     0	     1	     1	    0	    1	   0	   0
문서3	 0	     1	     1	     0	     2	    0	    0	   0	   0
문서4	 1	     0	     0	     0	     0	    0	    0	   1	   1

이걸로 예를 들어 보겠다.
TF는 앞서 사용한 DTM을 그대로 사용하면 된다.

구해야할 것은 TF와 곱해야할 값이 IDF입니다. 로그는 자연 로그를 사용한다. 자연 로그는 로그의 밑을 자연상수 e(2.718281...)를 사용하는 로그를 말한다. IDF 계산을 위해 사용하는 로그의 밑은 TF-IDF를 사용하는 사용자가 임의로 정할 수 있는데, 여기서 로그는 마치 기존의 값에 곱하여 값의 크기를 조절하는 상수의 역할을 합니다. 그런데 보통 각종 프로그래밍 언어나 프로그램에서 패키지로 지원하는 TF-IDF의 로그는 대부분 자연 로그를 사용한다. 그렇기 때문에 자연로그를 사용. 자연 로그는 보통 log라고 표현하지 않고 ln이라고 표현

단어	IDF(역 문서 빈도)
과일이	ln(4/(1+1)) = 0.693147
길고	ln(4/(1+1)) = 0.693147
노란	ln(4/(1+1)) = 0.693147
먹고	ln(4/(2+1)) = 0.287682
바나나	ln(4/(2+1)) = 0.287682
사과	ln(4/(1+1)) = 0.693147
싶은	ln(4/(2+1)) = 0.287682
저는	ln(4/(1+1)) = 0.693147
좋아요	ln(4/(1+1)) = 0.693147

n/ 1+df(t) 이기 때문에 전체문서 / 1+ 단어의 전체 문서에서의 등장횟수를 해서 ln을 하면 IDF가 나온다.

문서의 총 수는 4이기 때문에 ln 안에서 분자는 늘 4로 동일하다. 분모의 경우에는 각 단어가 등장한 문서의 수(DF)를 의미하는데, 예를 들어서 '먹고'의 경우에는 총 2개의 문서(문서1, 문서2)에 등장했기 때문에 2라는 값을 가진다. 각 단어에 대해서 IDF의 값을 비교해보면 문서 1개에만 등장한 단어와 문서2개에만 등장한 단어는 값의 차이를 보입니다. IDF는 여러 문서에서 등장한 단어의 가중치를 낮추는 역할을 하기 때문이다.

그러면 이제 TF-IDF를 계산해보도록 하겠습니다. TF-DTM을 그대로 가져오면 각 문서에서의 각 단어의 TF를 가져오게 되기 때문에, 앞서 사용한 DTM에서 단어 별로 위의 IDF값을 그대로 곱해주면 TF-IDF가 나오게 된다.


	    과일이	길고	    노란	먹고	바나나	    사과	   싶은	    저는	좋아요
문서1	    0	0	        0	 0.287682	0	    0.693147  0.287682	  0	       0
문서2	    0	0	        0	 0.287682	0.287682	0	  0.287682	  0   	   0
문서3	    0	0.693147 0.693147	0	    0.575364	0	    0	      0	       0
문서4	0.693147  0  	 0	    0	    0	        0	    0	    0.693147	0.693147

사실 예제 문서가 굉장히 간단하기 때문에 계산은 매우 쉽다. 문서3에서의 바나나만 TF 값이 2이므로 IDF에 2를 곱해주고, 나머지 TF 값이 1이므로 그대로 IDF 값을 가져오면 된다. 문서2에서의 바나나의 TF-IDF가중치와 문서3에서의 바나나의 TF-IDF 가중치가 다른 것을 볼 수 있다. 수식적으로 말하면, TF가 각각 1과 2로 달랐기 때문인데, TF-IDF에서의 관점에서 보자면 TF-IDF는 특정 문서에서 자주 등장하는 단어는 그 문서 내에서 중요한 단어로 판단하기 때문이다. 문서2에서는 바나나를 한 번 언급했지만, 문서 3에서는 바나나를 두 번 언급 했기 때문에 문서3에서의 바나나를 더욱 중요한 단어라고 판단하는 것입니다.

## 사이킷런을 이용한 DTM과 TF-IDF 실습

실습을 통해 DTM과 TF-IDF를 직접 만들어본다. DTM 또한 BoW 행렬이기 때문에, 앞서 BoW 챕터에서 배운 CountVectorizer를 이용하면 간단히 DTM을 만들 수 있다.

	from sklearn.feature_extraction.text import CountVectorizer

	corpus = [
		'you know I want your love',
		'I like you',
		'what should I do',
	]

	vector = CountVectorizer()
	print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도수를 기록.
	print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.

하면

	[[0 1 0 1 0 1 0 1 1]
 	[0 0 1 0 0 0 0 1 0]
	[1 0 0 0 1 0 1 0 0]]

	{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}

이런 결과가 나온다. 
DTM이 완성 되고 . DTM에서 각 단어의 인덱스가 어떻게 부여되었는지를 확인할 수 있다. 첫 번째 열의 경우에는 0의 인덱스를 가진 do.이다 do는 세번째 문서에만 등장했기 때문에, 세번째 행에서만 1의 값을 가진다. 두 번째 열의 경우에는 1의 인덱스를 가진 know이다. know는 첫번째 문서에만 등장했기 때문에 첫 번째 행에서만 1의 값을 가진다.

사이킷런은 TF-IDF를 자동 계산해주는 TfidfVectorize를 제공한다. 
사이킷런의 TF-IDF는 위에서 배웠던 보편적인 TF-IDF 식에서 좀 더 조정된 다른 식을 사용한다. 하지만 크게 다른식은 아니며 (L2 정규화 적용), 여전히 TF-IDF가 가진 의도를 그대로 가지고 있으므로 사이킷런의 TF-IDF를 그대로 사용해도 좋다.

	from sklearn.feature_extraction.text import TfidfVectorizer

	corpus = [
		'you know I want your love',
		'I like you',
		'what should I do',
	]

	tfidfv = TfidfVectorizer().fit(corpus)
	print(tfidfv.tarnsform(corpus).toarray())
	print(tfidfv.vocabulary_)

를 하면 

	[
		[0.         0.46735098 0.         0.46735098 0.         0.46735098 0.         0.35543247 0.46735098]
 		[0.         0.         0.79596054 0.         0.         0.         0.         0.60534851 0.        ]
 		[0.57735027 0.         0.         0.         0.57735027 0.         0.57735027 0.         0.        ]
		 ]
	{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}

이런식으로 나온다. 큰 이중 리스트로 되어있고 각각의 단어 인덱스와 TF-IDF 점수가 나온다.